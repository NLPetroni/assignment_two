{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "solution_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNVTPlHolp3u2i/mNEKubSw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NLPetroni/assignment_two/blob/main/solution_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ldIOU6KPl4"
      },
      "source": [
        "# Imports and downloads\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpI5pddAJRU5",
        "outputId": "250e9091-2c87-49e4-ef3c-79f18dd766cc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "\n",
        "%cd /content\n",
        "!rm -rf assignment_two &> /dev/null\n",
        "!git clone https://github.com/NLPetroni/assignment_two &> /dev/null\n",
        "%cd assignment_two\n",
        "sys.path.append(os.getcwd())\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/assignment_two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2uzDyzCKrGy"
      },
      "source": [
        "from src import utils\n",
        "import re\n",
        "from functools import reduce\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from typing import List, Callable, Dict\n",
        "import random\n",
        "import torch"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zr8CGx9JmRN"
      },
      "source": [
        "utils.download_data('dataset')\n",
        "train_set = pd.read_csv(\"dataset/train_pairs.csv\")\n",
        "val_set = pd.read_csv(\"dataset/val_pairs.csv\")\n",
        "test_set = pd.read_csv(\"dataset/test_pairs.csv\")"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ59KJhcJr5I",
        "outputId": "7002762d-8cc8-43a8-fe94-8ef8200b7509"
      },
      "source": [
        "print(train_set.columns)\n",
        "print(\"Total rows of the train set: {:d}\".format(len(train_set)))\n",
        "print(\"Total rows of the validation set: {:d}\".format(len(val_set)))\n",
        "print(\"Total rows of the test set: {:d}\".format(len(test_set)))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'Claim', 'Evidence', 'ID', 'Label'], dtype='object')\n",
            "Total rows of the train set: 121740\n",
            "Total rows of the validation set: 7165\n",
            "Total rows of the test set: 7189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTfRZo0zJv16",
        "outputId": "76f29416-6115-4731-ec96-f1126da2be1b"
      },
      "source": [
        "train_set['Label'].value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SUPPORTS    89389\n",
              "REFUTES     32351\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBtVzSl_J0CR"
      },
      "source": [
        "# Dataset pre-processing and conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mps78KxwJ1e6"
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\t-]')\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "BAD_SYMBOLS_RE = re.compile('(-LRB-)|(-RRB-)|(-LSB-)|(-RSB-)')\n",
        "INSIDE_SQAURE_BRACKETS_RE = re.compile('(-LSB-).*?(-RSB-)')\n",
        "\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def remove_inside_square_brackets(text: str) -> str:\n",
        "    return INSIDE_SQAURE_BRACKETS_RE.sub('', text)\n",
        "\n",
        "def remove_bad_symbols(text: str) -> str:\n",
        "    return BAD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_final_tags(text: str) -> str:\n",
        "   return re.sub('\\.\\t.*?$', '', text) \n",
        "\n",
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def replace_br(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces br characters\n",
        "    \"\"\"\n",
        "\n",
        "    return text.replace('</br>', '')\n",
        "\n",
        "def filter_out_uncommon_symbols(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def split_text(text: str) -> List:\n",
        "  return text.split()\n",
        "\n",
        "PREPROCESSING_PIPELINE = [\n",
        "                          remove_inside_square_brackets,\n",
        "                          remove_bad_symbols,\n",
        "                          lower,\n",
        "                          remove_final_tags,\n",
        "                          replace_special_characters,\n",
        "                          filter_out_uncommon_symbols,\n",
        "                          remove_stopwords,\n",
        "                          strip_text,\n",
        "                          split_text\n",
        "                          ]\n",
        "\n",
        "# Anchor method\n",
        "\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "\n",
        "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "\n",
        "# In the evidences there is an id at the beginning of the sequence which is\n",
        "# removed with the splice [:1]\n",
        "train_set['Evidence'] = train_set['Evidence'].apply(lambda txt: text_prepare(txt)[1:])\n",
        "train_set['Claim'] = train_set['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "val_set['Evidence'] = val_set['Evidence'].apply(lambda txt: text_prepare(txt)[1:])\n",
        "val_set['Claim'] = val_set['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "test_set['Evidence'] = test_set['Evidence'].apply(lambda txt: text_prepare(txt)[1:])\n",
        "test_set['Claim'] = test_set['Claim'].apply(lambda txt: text_prepare(txt))"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddUYDxxuJ6oG",
        "outputId": "0b065604-a3cf-493c-8c0a-78e61fb63339"
      },
      "source": [
        "print(\"Before: \"+str(train_copy.iloc[0]['Evidence']))\n",
        "print(\"After: \"+str(train_set.iloc[0]['Evidence']))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: 2\tHemsworth has also appeared in the science fiction action film Star Trek -LRB- 2009 -RRB- , the thriller adventure A Perfect Getaway -LRB- 2009 -RRB- , the horror comedy The Cabin in the Woods -LRB- 2012 -RRB- , the dark-fantasy action film Snow White and the Huntsman -LRB- 2012 -RRB- , the war film Red Dawn -LRB- 2012 -RRB- , and the biographical sports drama film Rush -LRB- 2013 -RRB- .\tStar Trek\tStar Trek (film)\tA Perfect Getaway\tA Perfect Getaway\tThe Cabin in the Woods\tThe Cabin in the Woods\tSnow White and the Huntsman\tSnow White and the Huntsman\tRed Dawn\tRed Dawn (2012 film)\tRush\tRush (2013 film)\n",
            "After: ['hemsworth', 'also', 'appeared', 'science', 'fiction', 'action', 'film', 'star', 'trek', '2009', 'thriller', 'adventure', 'perfect', 'getaway', '2009', 'horror', 'comedy', 'cabin', 'woods', '2012', 'dark', 'fantasy', 'action', 'film', 'snow', 'white', 'huntsman', '2012', 'war', 'film', 'red', 'dawn', '2012', 'biographical', 'sports', 'drama', 'film', 'rush', '2013']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XUEfrJOhEQZ"
      },
      "source": [
        "voc_evidence = [item for sublist in train_set[:]['Evidence'] for item in sublist]\n",
        "voc_claim = [item for sublist in train_set[:]['Claim'] for item in sublist]\n",
        "vocabulary = list(set(voc_evidence + voc_claim))\n",
        "\n",
        "def tokenize(input: List) -> torch.Tensor:\n",
        "  result = list(map(lambda x: vocabulary.index(x), input))\n",
        "  return torch.tensor(result)\n",
        "\n",
        "def detokenize(input: torch.Tensor) -> List:\n",
        "  result = input.tolist()\n",
        "  result = list(map(lambda x: vocabulary[x], result))\n",
        "  return result"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vcu8TTVqRLp"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rpXu4uhqcnH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}