{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "solution_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMdN1gTucqm71FuUscYe0C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NLPetroni/assignment_two/blob/main/solution_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ldIOU6KPl4"
      },
      "source": [
        "# Imports and downloads\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpI5pddAJRU5",
        "outputId": "ecd6986b-6041-4fd2-d829-3cd214bc709d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "\n",
        "%cd /content\n",
        "!rm -rf assignment_two &> /dev/null\n",
        "!git clone https://github.com/NLPetroni/assignment_two &> /dev/null\n",
        "%cd assignment_two\n",
        "sys.path.append(os.getcwd())\n",
        "!git clone https://gitlab.com/sasso-effe/nlp-assignment-data.git &> /dev/null\n",
        "!mv nlp-assignment-data/embedding_matrix.npy res/embedding_matrix.npy\n",
        "!rm -rf nlp-assignment-data\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/assignment_two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2uzDyzCKrGy"
      },
      "source": [
        "from src import utils\n",
        "import re\n",
        "from functools import reduce\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from typing import List, Callable, Dict\n",
        "import random\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zr8CGx9JmRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd324c51-8495-4b3c-cae3-e203108be5f2"
      },
      "source": [
        "utils.download_data('dataset')\n",
        "train_set = pd.read_csv(\"dataset/train_pairs.csv\")\n",
        "val_set = pd.read_csv(\"dataset/val_pairs.csv\")\n",
        "test_set = pd.read_csv(\"dataset/test_pairs.csv\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading FEVER data splits...\n",
            "Download completed!\n",
            "Extracting dataset...\n",
            "Extraction completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ59KJhcJr5I",
        "outputId": "ced42a19-7dcb-4bfa-fe03-2400f68aa6ce"
      },
      "source": [
        "print(train_set.columns)\n",
        "print(\"Total rows of the train set: {:d}\".format(len(train_set)))\n",
        "print(\"Total rows of the validation set: {:d}\".format(len(val_set)))\n",
        "print(\"Total rows of the test set: {:d}\".format(len(test_set)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'Claim', 'Evidence', 'ID', 'Label'], dtype='object')\n",
            "Total rows of the train set: 121740\n",
            "Total rows of the validation set: 7165\n",
            "Total rows of the test set: 7189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTfRZo0zJv16",
        "outputId": "0fa2f775-83b0-4b39-bd19-733d8e334bdb"
      },
      "source": [
        "train_set['Label'].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SUPPORTS    89389\n",
              "REFUTES     32351\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib-Br_fNRhVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ca260d-673e-46b7-ba33-c11dcc0a3b8e"
      },
      "source": [
        "print(train_set.iloc[0]['Evidence'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\tHemsworth has also appeared in the science fiction action film Star Trek -LRB- 2009 -RRB- , the thriller adventure A Perfect Getaway -LRB- 2009 -RRB- , the horror comedy The Cabin in the Woods -LRB- 2012 -RRB- , the dark-fantasy action film Snow White and the Huntsman -LRB- 2012 -RRB- , the war film Red Dawn -LRB- 2012 -RRB- , and the biographical sports drama film Rush -LRB- 2013 -RRB- .\tStar Trek\tStar Trek (film)\tA Perfect Getaway\tA Perfect Getaway\tThe Cabin in the Woods\tThe Cabin in the Woods\tSnow White and the Huntsman\tSnow White and the Huntsman\tRed Dawn\tRed Dawn (2012 film)\tRush\tRush (2013 film)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBtVzSl_J0CR"
      },
      "source": [
        "# Dataset pre-processing and conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mps78KxwJ1e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d32ef1-6000-4326-d7ce-680a90f9a02b"
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\t-]')\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "BAD_SYMBOLS_RE = re.compile('(-LRB-)|(-RRB-)|(-LSB-)|(-RSB-)')\n",
        "INSIDE_SQAURE_BRACKETS_RE = re.compile('(-LSB-).*?(-RSB-)')\n",
        "\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def remove_inside_square_brackets(text: str) -> str:\n",
        "    return INSIDE_SQAURE_BRACKETS_RE.sub('', text)\n",
        "\n",
        "def remove_bad_symbols(text: str) -> str:\n",
        "    return BAD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_final_tags(text: str) -> str:\n",
        "   return re.sub('\\.\\t.*?$', '', text) \n",
        "\n",
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def replace_br(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces br characters\n",
        "    \"\"\"\n",
        "\n",
        "    return text.replace('</br>', '')\n",
        "\n",
        "def filter_out_uncommon_symbols(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def split_text(text: str) -> List:\n",
        "  return text.split()\n",
        "\n",
        "PREPROCESSING_PIPELINE = [\n",
        "                          remove_inside_square_brackets,\n",
        "                          remove_bad_symbols,\n",
        "                          lower,\n",
        "                          remove_final_tags,\n",
        "                          replace_special_characters,\n",
        "                          filter_out_uncommon_symbols,\n",
        "                          remove_stopwords,\n",
        "                          strip_text,\n",
        "                          split_text\n",
        "                          ]\n",
        "\n",
        "# Anchor method\n",
        "\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "\n",
        "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "\n",
        "# In the evidences there is an id at the beginning of the sequence which is\n",
        "# removed with the splice [:1]\n",
        "train_set['Evidence'] = train_set['Evidence'].apply(lambda txt: text_prepare(txt)[1:])\n",
        "train_set['Claim'] = train_set['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "val_set['Evidence'] = val_set['Evidence'].apply(lambda txt: text_prepare(txt)[1:])\n",
        "val_set['Claim'] = val_set['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "test_set['Evidence'] = test_set['Evidence'].apply(lambda txt: text_prepare(txt)[1:])\n",
        "test_set['Claim'] = test_set['Claim'].apply(lambda txt: text_prepare(txt))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XUEfrJOhEQZ"
      },
      "source": [
        "voc_evidence = [item for sublist in train_set[:]['Evidence'] for item in sublist]\n",
        "voc_claim = [item for sublist in train_set[:]['Claim'] for item in sublist]\n",
        "vocabulary = list(set(voc_evidence + voc_claim))\n",
        "\n",
        "def tokenize(input: List) -> torch.Tensor:\n",
        "  result = list(map(lambda x: vocabulary.index(x), input))\n",
        "  return torch.tensor(result)\n",
        "\n",
        "def detokenize(input: torch.Tensor) -> List:\n",
        "  result = input.tolist()\n",
        "  result = list(map(lambda x: vocabulary[x], result))\n",
        "  return result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo02GPAtUBlt"
      },
      "source": [
        "## Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbq8J24TXLdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b934a906-880a-4b85-f5a9-e1aca01e3391"
      },
      "source": [
        "import pickle\n",
        "\n",
        "if (os.path.exists(\"res/vocabulary.pkl\") and os.path.exists(\"res/embedding_matrix.npy\")):\n",
        "  print('The vocabulary and the embedding matrix are already present. Loading them...')\n",
        "  with open('res/vocabulary.pkl', 'rb') as f:\n",
        "    VOCABULARY = pickle.load(f)\n",
        "  EMBEDDING_MATRIX = np.load(\"res/embedding_matrix.npy\")\n",
        "  print(\"Done!\")\n",
        "  \n",
        "else:\n",
        "  print(\"The vocabulary and the embedding matrix are NOT present. Creating them...\")\n",
        "  TRAIN_VOC = set(vocabulary)\n",
        "  voc_evidence = [item for sublist in val_set[:]['Evidence'] for item in sublist]\n",
        "  voc_claim = [item for sublist in val_set[:]['Claim'] for item in sublist]\n",
        "  VAL_VOC = set(voc_evidence + voc_claim)\n",
        "\n",
        "  inputs = train_set[:]['Evidence'].tolist() + train_set[:]['Claim'].tolist()\n",
        "  glove_voc, embedding_matrix = utils.get_glove(number_token=False)\n",
        "  vocabulary, embedding_matrix = utils.add_oov(glove_voc, TRAIN_VOC, embedding_matrix, inputs)\n",
        "  inputs = val_set[:]['Evidence'].tolist() + val_set[:]['Claim'].tolist()\n",
        "  vocabulary, embedding_matrix = utils.add_oov(vocabulary, VAL_VOC, embedding_matrix, inputs)\n",
        "\n",
        "  with open(\"vocabulary.pkl\", \"wb\") as file:\n",
        "    pickle.dump(vocabulary, file)\n",
        "  np.save(\"embedding_matrix.npy\", embedding_matrix)\n",
        "  print(\"Vocabulary and embedding matrix created! Remember to download the generated files.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary and the embedding matrix are already present. Loading them...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vcu8TTVqRLp"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Hk8G5NcuS4"
      },
      "source": [
        "def get_classifier(name:str,\n",
        "                    w_in: int,\n",
        "                    w_hidden: int,\n",
        "                    w_out: int) -> nn.Sequential:\n",
        "    \"\"\"Gets a sequential container with a linear+relu+linear classifier\n",
        "\n",
        "    Args:\n",
        "        name: the name prefix to append to each layer in the container.\n",
        "        w_in: the number of the input features.\n",
        "        w_hidden: the number of internal weights\n",
        "        w_out: the number of the output features.\n",
        "\n",
        "    Returns: the created sequential.\n",
        "    \"\"\"\n",
        "    container = nn.Sequential()\n",
        "    container.add_module(f'{name}_fc1', nn.Linear(in_features=w_in, out_features=w_hidden))    \n",
        "    container.add_module(f'{name}_ReLU', nn.ReLU(inplace=True))\n",
        "    container.add_module(f'{name}_fc2', nn.Linear(in_features=w_hidden, out_features=w_out))    \n",
        "    return container\n",
        "\n",
        " \n",
        "class RNNEncoder(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, num_layers, rnn_type='elman', output_state='last'):\n",
        "    super().__init__()\n",
        "    types = {'elman': nn.RNN, 'lstm': nn.LSTM, 'gru': nn.GRU}\n",
        "    states = {\n",
        "        'last': lambda x: x[-1],\n",
        "        'avg': lambda x: torch.mean(x, dim=0)}\n",
        "    \n",
        "    try:\n",
        "      self.output_state_fn = states[output_state]\n",
        "    except:\n",
        "      valid_states = list(states.keys())\n",
        "      raise ValueError(f\"wrong type '{output_state}', must be in {valid_states}\")\n",
        "\n",
        "    try:\n",
        "      rec_module = types[rnn_type]\n",
        "    except:\n",
        "      valid_types = list(types.keys())\n",
        "      raise ValueError(f\"wrong type '{rnn_type}', must be in {valid_types}\")\n",
        "    self.rec_module = rec_module(input_size=input_size, hidden_size=hidden_size,\n",
        "                                 bidirectional=True, batch_first=False,\n",
        "                                 num_layers=num_layers)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    output, _ = self.rec_module(x)\n",
        "    return self.output_state_fn(output)\n",
        "\n",
        "class BagOfVectorsEncoder(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TODO: check if the mean is computed on the right axis\n",
        "    return torch.mean(x, axis=1)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rpXu4uhqcnH"
      },
      "source": [
        "class FactChecker(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_matrix, encoder, merger, rnn_type=None, rnn_output=None, rec_size=1, hid_size=50):\n",
        "    \"\"\"\n",
        "      A recurrent network performing Neural Language Inference (Fact Checking).\n",
        "      Params:\n",
        "        embedding_matrix: the embedding matrix for word embedding\n",
        "        encoder: [rnn, mlp, bag], the encoder to compute the sentence embedding\n",
        "        merger: [concatenation, sum, mean], the multi-input merging strategy\n",
        "        RNNEncoder params, only relevant if encoder==rnn:\n",
        "          rnn_type: [elman, lstm, gru], the RNN architecure used in the encoder\n",
        "          rnn_output: [last, avg], the function to compute the sentence encoding from the RNN hidden states\n",
        "          rec_size: int, the number of layers in the rnn\n",
        "          hid_size: int, the hidden size of the rnn\n",
        "\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.hid_size = hid_size\n",
        "\n",
        "    # Word embedding\n",
        "    emb_size = embedding_matrix.shape[1]\n",
        "    self.emb_layer = nn.Embedding.from_pretrained(torch.as_tensor(embedding_matrix))\n",
        "\n",
        "    # Sentence embedding\n",
        "    if encoder == 'rnn':\n",
        "      self.encoder = RNNEncoder(emb_size, hid_size, rec_size, rnn_type=rnn_type, output_state=rnn_output)\n",
        "    elif encoder == 'mlp':\n",
        "      pass #TODO: implement\n",
        "    elif encoder == 'bag':\n",
        "      self.encoder = BagOfVectorsEncoder()\n",
        "    else:\n",
        "      raise ValueError(f\"Wrong encoder '{encoder}', must be in ['rnn', 'mlp', 'bag']\")\n",
        "\n",
        "    # Merging\n",
        "    merging_strategies = {\n",
        "        # TODO: check that these function operate on the right axis\n",
        "        'concatenation': torch.cat,\n",
        "        'sum': torch.add,\n",
        "        'mean': torch.mean\n",
        "    }\n",
        "    try:\n",
        "      merging_fn = merging_strategies[merger]\n",
        "    except:\n",
        "      valid_strategies = list(merging_strategies.keys())\n",
        "      raise ValueError(f\"wrong type '{merger}', must be in {valid_strategies}\")\n",
        "    self.merger = merging_fn\n",
        "\n",
        "    # Classifier\n",
        "    self.classifier = get_classifier('classifier', w_in=2*self.hid_size, w_hidden=self.hid_size, w_out=2)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    vecs = self.emb_layer(x).float()\n",
        "    output = self.encoder(vecs)\n",
        "    # it works until here (probably)\n",
        "    return output\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPTzVP8MTNx4"
      },
      "source": [
        "model = FactChecker(EMBEDDING_MATRIX, 'rnn', 'sum', rnn_type='elman', rnn_output='last', rec_size=1, hid_size=50)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Nf8wiFgU_h0",
        "outputId": "09a76fdc-6231-4575-9afe-3f5283b47b1d"
      },
      "source": [
        "x = torch.randint(2000, (10,10))\n",
        "print(model(x).shape)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GDv0pqw3UsB"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWtu6Umy3UPr"
      },
      "source": [
        "def training_step(model, optimizer, loss_fn, data_loader, device):\n",
        "  model.train()\n",
        "\n",
        "  for (input, target) in data_loader:\n",
        "    #forward\n",
        "    input = input.to(device)\n",
        "    target = target.to(device)\n",
        "    output = model(input)\n",
        "    loss = loss_fn(output, target)\n",
        "    loss_value = loss.item()\n",
        "\n",
        "    if not math.isfinite(loss_value):\n",
        "      print(f\"Loss is {loss_value}, stopping training\")\n",
        "      exit(1)\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em6r40Wq7NWn"
      },
      "source": [
        "def train(optimizer_name, lr, loss_fn, data_loader, device, n_epochs, verbose, batch_size):\n",
        "  '''wandb.login(key=utils.get_wandbkey()) # TODO: implement getter\n",
        "  run = wandb.init(project=\"assignment-two\", entity=\"nlpetroni\", reinit=True, config=cfg_dict)\n",
        "  wandb.define_metric(\"train_step\")\n",
        "  wandb.define_metric(\"epoch\")\n",
        "  wandb.define_metric('train/loss', step_metric=\"train_step\", summary=\"min\")\n",
        "  wandb.define_metric(\"valid/loss\", step_metric=\"epoch\", summary=\"min\")\n",
        "  wandb.define_metric(\"valid/accuracy\", step_metric=\"epoch\", summary=\"max\")'''\n",
        "\n",
        "  train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "  valid_dl = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
        "\n",
        "  model = FactChecker(EMBEDDING_MATRIX, 'rnn', rnn_type='lstm', rnn_output='last')\n",
        "  wandb.watch(model, log_graph=True)\n",
        "  if verbose:\n",
        "    print(summary(model))\n",
        "\n",
        "  if optimizer_name == 'rmsprop':\n",
        "    optimizer = torch.optim.RMSprop(params, lr=cfg.LR, alpha=0.99, momentum=0.5, weight_decay=0)\n",
        "  elif optimizer_name == 'adam':\n",
        "    optimizer = torch.optim.Adam(params, lr=cfg.LR, betas=(0.9, 0.999), weight_decay=0)\n",
        "  else:\n",
        "    raise ValueError(f'wrong optim {optimizer_name}, either rmsprop or adam')\n",
        "\n",
        "  loss = nn.NLLLoss()\n",
        "  train_step = 0\n",
        "  print('STARTING TRAINING')\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    training_step(model, optimizer, loss, train_dl, device)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJP2oodB-kS5"
      },
      "source": [
        "v, m = train(0, 0, 0, 0, 0, 0)\n",
        "print(list(m.keys())[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erLtxDfbGeMV"
      },
      "source": [
        "print(m[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CshUGu0OJOrg"
      },
      "source": [
        "inputs = train_set[:]['Evidence'].tolist() + train_set[:]['Claim'].tolist()\n",
        "print(inputs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikpf3WelLDGF"
      },
      "source": [
        "glove_voc = utils.get_glove(number_token=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv1Hc1UuLP1I"
      },
      "source": [
        "list(glove_voc[0].keys())[0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}