{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "solution_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPKVikswIH0OpL2hIsgJuN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NLPetroni/assignment_two/blob/main/solution_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ldIOU6KPl4"
      },
      "source": [
        "# Imports and downloads\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpI5pddAJRU5",
        "outputId": "b94c0bc4-ddb5-4407-af90-1d74b03a1e7f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "\n",
        "%cd /content\n",
        "!rm -rf assignment_two &> /dev/null\n",
        "!git clone https://github.com/NLPetroni/assignment_two &> /dev/null\n",
        "%cd assignment_two\n",
        "sys.path.append(os.getcwd())\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/assignment_two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2uzDyzCKrGy"
      },
      "source": [
        "from src import utils\n",
        "import re\n",
        "from functools import reduce\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from typing import List, Callable, Dict\n",
        "import random\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zr8CGx9JmRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede70f37-19ad-48a6-d0d8-21ccb4b941d4"
      },
      "source": [
        "utils.download_data('dataset')\n",
        "train_set = pd.read_csv(\"dataset/train_pairs.csv\")\n",
        "val_set = pd.read_csv(\"dataset/val_pairs.csv\")\n",
        "test_set = pd.read_csv(\"dataset/test_pairs.csv\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading FEVER data splits...\n",
            "Download completed!\n",
            "Extracting dataset...\n",
            "Extraction completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ59KJhcJr5I",
        "outputId": "5d93e9e3-b760-4e21-8d8a-2574e976068c"
      },
      "source": [
        "print(train_set.columns)\n",
        "print(\"Total rows of the train set: {:d}\".format(len(train_set)))\n",
        "print(\"Total rows of the validation set: {:d}\".format(len(val_set)))\n",
        "print(\"Total rows of the test set: {:d}\".format(len(test_set)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'Claim', 'Evidence', 'ID', 'Label'], dtype='object')\n",
            "Total rows of the train set: 121740\n",
            "Total rows of the validation set: 7165\n",
            "Total rows of the test set: 7189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTfRZo0zJv16",
        "outputId": "5cfdb8a3-7f68-4f87-8a8d-9dcd30dcd56d"
      },
      "source": [
        "train_set['Label'].value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SUPPORTS    89389\n",
              "REFUTES     32351\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib-Br_fNRhVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93e56e1-42da-4563-d48d-d8c3387dfda8"
      },
      "source": [
        "print(train_set.iloc[0]['Evidence'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\tHemsworth has also appeared in the science fiction action film Star Trek -LRB- 2009 -RRB- , the thriller adventure A Perfect Getaway -LRB- 2009 -RRB- , the horror comedy The Cabin in the Woods -LRB- 2012 -RRB- , the dark-fantasy action film Snow White and the Huntsman -LRB- 2012 -RRB- , the war film Red Dawn -LRB- 2012 -RRB- , and the biographical sports drama film Rush -LRB- 2013 -RRB- .\tStar Trek\tStar Trek (film)\tA Perfect Getaway\tA Perfect Getaway\tThe Cabin in the Woods\tThe Cabin in the Woods\tSnow White and the Huntsman\tSnow White and the Huntsman\tRed Dawn\tRed Dawn (2012 film)\tRush\tRush (2013 film)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBtVzSl_J0CR"
      },
      "source": [
        "# Dataset pre-processing and conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mps78KxwJ1e6"
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;\\t-]')\n",
        "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "BAD_SYMBOLS_RE = re.compile('(-LRB-)|(-RRB-)|(-LSB-)|(-RSB-)')\n",
        "INSIDE_SQAURE_BRACKETS_RE = re.compile('(-LSB-).*?(-RSB-)')\n",
        "\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def remove_inside_square_brackets(text: str) -> str:\n",
        "    return INSIDE_SQAURE_BRACKETS_RE.sub('', text)\n",
        "\n",
        "def remove_bad_symbols(text: str) -> str:\n",
        "    return BAD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_final_tags(text: str) -> str:\n",
        "   return re.sub('\\.\\t.*?$', '', text) \n",
        "\n",
        "def lower(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Transforms given text to lower case.\n",
        "    Example:\n",
        "    Input: 'I really like New York city'\n",
        "    Output: 'i really like new your city'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "def replace_special_characters(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces special characters, such as paranthesis,\n",
        "    with spacing character\n",
        "    \"\"\"\n",
        "\n",
        "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "\n",
        "def replace_br(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces br characters\n",
        "    \"\"\"\n",
        "\n",
        "    return text.replace('</br>', '')\n",
        "\n",
        "def filter_out_uncommon_symbols(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any special character that is not in the\n",
        "    good symbols list (check regular expression)\n",
        "    \"\"\"\n",
        "\n",
        "    return GOOD_SYMBOLS_RE.sub('', text)\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
        "\n",
        "\n",
        "def strip_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes any left or right spacing (including carriage return) from text.\n",
        "    Example:\n",
        "    Input: '  This assignment is cool\\n'\n",
        "    Output: 'This assignment is cool'\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def split_text(text: str) -> List:\n",
        "  return text.split()\n",
        "\n",
        "PREPROCESSING_PIPELINE = [\n",
        "                          remove_inside_square_brackets,\n",
        "                          remove_bad_symbols,\n",
        "                          lower,\n",
        "                          remove_final_tags,\n",
        "                          replace_special_characters,\n",
        "                          filter_out_uncommon_symbols,\n",
        "                          remove_stopwords,\n",
        "                          strip_text,\n",
        "                          split_text\n",
        "                          ]\n",
        "\n",
        "# Anchor method\n",
        "\n",
        "def text_prepare(text: str,\n",
        "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
        "    \"\"\"\n",
        "    Applies a list of pre-processing functions in sequence (reduce).\n",
        "    Note that the order is important here!\n",
        "    \"\"\"\n",
        "\n",
        "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
        "\n",
        "    return reduce(lambda txt, f: f(txt), filter_methods, text)\n",
        "\n",
        "\n",
        "# In the evidences there is an id at the beginning of the sequence which is\n",
        "# removed with the splice [:1]\n",
        "train_set['Evidence'] = train_set['Evidence'].apply(lambda txt: text_prepare(txt)[1:])\n",
        "train_set['Claim'] = train_set['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "val_set['Evidence'] = val_set['Evidence'].apply(lambda txt: text_prepare(txt)[1:])\n",
        "val_set['Claim'] = val_set['Claim'].apply(lambda txt: text_prepare(txt))\n",
        "\n",
        "test_set['Evidence'] = test_set['Evidence'].apply(lambda txt: text_prepare(txt)[1:])\n",
        "test_set['Claim'] = test_set['Claim'].apply(lambda txt: text_prepare(txt))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XUEfrJOhEQZ"
      },
      "source": [
        "voc_evidence = [item for sublist in train_set[:]['Evidence'] for item in sublist]\n",
        "voc_claim = [item for sublist in train_set[:]['Claim'] for item in sublist]\n",
        "vocabulary = list(set(voc_evidence + voc_claim))\n",
        "\n",
        "def tokenize(input: List) -> torch.Tensor:\n",
        "  result = list(map(lambda x: vocabulary.index(x), input))\n",
        "  return torch.tensor(result)\n",
        "\n",
        "def detokenize(input: torch.Tensor) -> List:\n",
        "  result = input.tolist()\n",
        "  result = list(map(lambda x: vocabulary[x], result))\n",
        "  return result"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo02GPAtUBlt"
      },
      "source": [
        "## Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbq8J24TXLdm"
      },
      "source": [
        "import pickle\n",
        "\n",
        "if (os.path.exists(\"res/vocabulary.pkl\") and os.path.exists(\"res/embedding_matrix.npy\")):\n",
        "  with open('res/vocabulary.pkl', 'rb') as f:\n",
        "    VOCABULARY = pickle.load(f)\n",
        "  EMBEDDING_MATRIX = np.load(\"res/embedding_matrix.npy\")\n",
        "  \n",
        "else:\n",
        "  TRAIN_VOC = set(vocabulary)\n",
        "  voc_evidence = [item for sublist in val_set[:]['Evidence'] for item in sublist]\n",
        "  voc_claim = [item for sublist in val_set[:]['Claim'] for item in sublist]\n",
        "  VAL_VOC = set(voc_evidence + voc_claim)\n",
        "\n",
        "  inputs = train_set[:]['Evidence'].tolist() + train_set[:]['Claim'].tolist()\n",
        "  glove_voc, embedding_matrix = utils.get_glove(number_token=False)\n",
        "  vocabulary, embedding_matrix = utils.add_oov(glove_voc, TRAIN_VOC, embedding_matrix, inputs)\n",
        "  inputs = val_set[:]['Evidence'].tolist() + val_set[:]['Claim'].tolist()\n",
        "  vocabulary, embedding_matrix = utils.add_oov(vocabulary, VAL_VOC, embedding_matrix, inputs)\n",
        "\n",
        "  with open(\"vocabulary.pkl\", \"wb\") as file:\n",
        "    pickle.dump(vocabulary, file)\n",
        "  np.save(\"embedding_matrix.npy\", embedding_matrix)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vcu8TTVqRLp"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Hk8G5NcuS4"
      },
      "source": [
        "def get_classifier(name:str,\n",
        "                    fc1_in: int,\n",
        "                    hidden: int\n",
        "                    fc2_out: int) -> nn.Sequential:\n",
        "    \"\"\"Gets a sequential container with a sandwich of Fully connected layer \n",
        "       and Relu.\n",
        "\n",
        "    Args:\n",
        "        name: the name prefix to append to each layer in the container.\n",
        "        features_in: the number of the input features.\n",
        "        features_out: the number of the output features.\n",
        "\n",
        "    Returns: the created sequential.\n",
        "    \"\"\"\n",
        "    container = nn.Sequential()\n",
        "    container.add_module(f'{name}_fc1', nn.Linear(in_features=fc1_in, out_features=hidden))    \n",
        "    container.add_module(f'{name}_ReLU', nn.ReLU(inplace=True))\n",
        "    container.add_module(f'{name}_fc2', nn.Linear(in_features=hidden, out_features=fc2_out))    \n",
        "    return container"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rpXu4uhqcnH"
      },
      "source": [
        "class RNNEncoder(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_matrix, type, rec_size=1, units=None, hid_size=50, state='last'):\n",
        "    \"\"\"\n",
        "      A recurrent network performing multiclass classification (POS tagging).\n",
        "      Params:\n",
        "        type: [elman, lstm, gru]\n",
        "        embedding_matrix: \n",
        "        rec_size: \n",
        "        units: \n",
        "        hid_size:\n",
        "        state: [avg, last]\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.state = state\n",
        "\n",
        "    emb_size = embedding_matrix.shape[1]\n",
        "    self.emb_layer = nn.Embedding.from_pretrained(torch.as_tensor(embedding_matrix))\n",
        "\n",
        "    if type == 'elman':\n",
        "      rec_module = nn.RNN\n",
        "    elif type == 'lstm':\n",
        "      rec_module = nn.LSTM\n",
        "    elif type == 'gru':\n",
        "      rec_module = nn.GRU\n",
        "    else:\n",
        "      raise ValueError(f'wrong type {type}, either elman, lstm or gru')\n",
        "    self.rec_modules = rec_module(input_size=emb_size, hidden_size=hid_size, bidirectional=True, batch_first=False, num_layers=rec_size)\n",
        "\n",
        "    self.classifier = get_classifier('classifier', fc1_in=2*self.hid_size, hidden=self.hid_size, fc2_out=2)\n",
        "    self.\n",
        "\n",
        "  def __call__(self, x):\n",
        "    vecs = self.emb_layer(x).float()\n",
        "    output, _ = self.rec_modules(vecs)\n",
        "    if self.state == 'last':\n",
        "      sentence_emb = output[-1]\n",
        "    if self.state == 'avg':\n",
        "      sentence_emb = torch.mean(output, dim=0)\n",
        "    # it works until here (probably)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GDv0pqw3UsB"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWtu6Umy3UPr"
      },
      "source": [
        "def training_step(model, optimizer, loss_fn, data_loader, device):\n",
        "  model.train()\n",
        "\n",
        "  for (input, target) in data_loader:\n",
        "    #forward\n",
        "    input = input.to(device)\n",
        "    target = target.to(device)\n",
        "    output = model(input)\n",
        "    loss = loss_fn(output, target)\n",
        "    loss_value = loss.item()\n",
        "\n",
        "    if not math.isfinite(loss_value):\n",
        "      print(f\"Loss is {loss_value}, stopping training\")\n",
        "      exit(1)\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()    \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em6r40Wq7NWn"
      },
      "source": [
        "def train(optimizer_name, lr, loss_fn, data_loader, device, n_epochs, verbose=False, batch_size):\n",
        "  '''wandb.login(key=utils.get_wandbkey()) # TODO: implement getter\n",
        "  run = wandb.init(project=\"assignment-two\", entity=\"nlpetroni\", reinit=True, config=cfg_dict)\n",
        "  wandb.define_metric(\"train_step\")\n",
        "  wandb.define_metric(\"epoch\")\n",
        "  wandb.define_metric('train/loss', step_metric=\"train_step\", summary=\"min\")\n",
        "  wandb.define_metric(\"valid/loss\", step_metric=\"epoch\", summary=\"min\")\n",
        "  wandb.define_metric(\"valid/accuracy\", step_metric=\"epoch\", summary=\"max\")'''\n",
        "\n",
        "  train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "  valid_dl = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
        "\n",
        "  model = RNNEncoder(EMBEDDING_MATRIX, type='lstm', state='last')\n",
        "  wandb.watch(model, log_graph=True)\n",
        "    if verbose:\n",
        "        print(summary(model))\n",
        "\n",
        "  if optimizer_name == 'rmsprop':\n",
        "    optimizer = torch.optim.RMSprop(params, lr=cfg.LR, alpha=0.99, momentum=0.5, weight_decay=0)\n",
        "  elif optimizer_name == 'adam':\n",
        "    optimizer = torch.optim.Adam(params, lr=cfg.LR, betas=(0.9, 0.999), weight_decay=0)\n",
        "  else:\n",
        "    raise ValueError(f'wrong optim {optimizer_name}, either rmsprop or adam')\n",
        "\n",
        "  loss = nn.NLLLoss()\n",
        "  train_step = 0\n",
        "  print('STARTING TRAINING')\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    training_step(model, optimizer, loss, train_dl, device)\n",
        "\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJP2oodB-kS5",
        "outputId": "508b63f9-6505-4ada-d15a-5927aa11ca4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "v, m = train(0, 0, 0, 0, 0, 0)\n",
        "print(list(m.keys())[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading and unzipping glove... completed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b8acb0298c59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erLtxDfbGeMV",
        "outputId": "8f66396d-0e33-47ef-f663-65b69ca08ecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(m[-1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.03331625 -0.308181    0.63416249 -0.5216718   0.98621252 -0.04922992\n",
            "  0.48023     0.1507365  -0.089796    0.05830625  0.66453999 -0.0792175\n",
            "  0.20350499  0.00856375 -0.03349375 -0.03475501  0.44441099 -0.00996525\n",
            " -0.0481375   0.20436675  0.72433324 -0.60414249  0.23826051  0.8045575\n",
            "  0.08928501  0.10531501  0.3549425  -0.20389625  0.6420625   0.33227601\n",
            " -0.17211001  0.1980715  -0.056469    0.48801751  0.08070225  0.26807705\n",
            " -0.06970499  0.48458    -0.35779    -0.50152176 -0.01007025  0.14516001\n",
            "  0.14906275  0.64062253  0.32606751 -0.17768801 -0.187397   -0.7296275\n",
            "  0.24034    -0.62037999 -0.14349001 -0.13984125 -0.05807375  0.63350751\n",
            " -0.40306251 -2.93017501 -0.31627426  0.00958225  1.10951     0.70182\n",
            " -0.0930165   0.58280248 -0.68472751 -0.20408176  0.93593749 -0.10864325\n",
            " -0.03932074  0.25197174  0.2498875  -0.34059025  0.02416751  0.66234\n",
            " -0.4861475   0.21644074 -0.09540675  0.64083499 -0.25014575 -0.085182\n",
            " -0.52128751 -0.53219751  0.21471     0.21045225 -0.158838    0.328684\n",
            " -1.00882998 -0.40932249  0.18130876 -0.58583499  0.04414499  0.4023475\n",
            " -0.30619499 -0.097865   -0.401195    0.2956625  -0.20414075  0.07679875\n",
            " -0.3388975   0.280305    0.48735501 -0.69820749]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CshUGu0OJOrg"
      },
      "source": [
        "inputs = train_set[:]['Evidence'].tolist() + train_set[:]['Claim'].tolist()\n",
        "print(inputs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikpf3WelLDGF"
      },
      "source": [
        "glove_voc = utils.get_glove(number_token=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv1Hc1UuLP1I"
      },
      "source": [
        "list(glove_voc[0].keys())[0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}